{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from load import load_pseudo, load_condons\n",
    "\n",
    "pd.options.display.precision = 3\n",
    "pd.options.display.max_colwidth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.64 s, sys: 92 ms, total: 4.74 s\n",
      "Wall time: 4.76 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>missing</th>\n",
       "      <th>missing_%</th>\n",
       "      <th>sequence_i</th>\n",
       "      <th>missing_i</th>\n",
       "      <th>missing_%_i</th>\n",
       "      <th>carb</th>\n",
       "      <th>toby</th>\n",
       "      <th>carb_num</th>\n",
       "      <th>toby_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>TA151</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>31842</td>\n",
       "      <td>6.588</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>28410</td>\n",
       "      <td>5.878</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>IC1</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>46071</td>\n",
       "      <td>9.532</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>34714</td>\n",
       "      <td>7.182</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A237</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>44514</td>\n",
       "      <td>9.210</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>35933</td>\n",
       "      <td>7.434</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5920</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>49497</td>\n",
       "      <td>10.241</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>36873</td>\n",
       "      <td>7.629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>LiA96</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>44067</td>\n",
       "      <td>9.117</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>34454</td>\n",
       "      <td>7.128</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   sequence  missing  missing_% sequence_i  missing_i  missing_%_i  \\\n",
       "0  TA151  ATGAGT...    31842      6.588  ATGAGT...      28410      5.878     \n",
       "1    IC1  ATGAGT...    46071      9.532  ATGAGT...      34714      7.182     \n",
       "2   A237  ATGAGT...    44514      9.210  ATGAGT...      35933      7.434     \n",
       "3   5920  ATGAGT...    49497     10.241  ATGAGT...      36873      7.629     \n",
       "4  LiA96  ATGAGT...    44067      9.117  ATGAGT...      34454      7.128     \n",
       "\n",
       "    carb   toby  carb_num  toby_num  \n",
       "0   True  False      -2.0      16.0  \n",
       "1  False  False       2.0      14.0  \n",
       "2   True  False      -1.0       4.0  \n",
       "3    NaN    NaN       NaN       NaN  \n",
       "4  False  False       0.0      18.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time records = load_pseudo()\n",
    "numerical_response = pd.read_csv('../data/pseudo/Perron_phenotype-GSU-training.csv')\n",
    "records = records.merge(numerical_response[['strain', 'carb.lag.delta', 'toby.lag.delta']],\n",
    "                        left_on='lab-id', right_on='strain', how='left')\n",
    "records.rename(columns={'carb.lag.delta': 'carb_num', 'toby.lag.delta': 'toby_num'}, inplace=True)\n",
    "records.drop(columns=['strain', 'lab-id'], inplace=True)\n",
    "records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (records['toby'].notna() & records['carb'].notna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22 s, sys: 956 ms, total: 22.9 s\n",
      "Wall time: 23 s\n",
      "CPU times: user 21.7 s, sys: 725 ms, total: 22.4 s\n",
      "Wall time: 22.4 s\n",
      "CPU times: user 21.6 s, sys: 415 ms, total: 22 s\n",
      "Wall time: 22.1 s\n",
      "CPU times: user 21.9 s, sys: 343 ms, total: 22.2 s\n",
      "Wall time: 22.3 s\n"
     ]
    }
   ],
   "source": [
    "# 22 seconds\n",
    "%time o_c = load_condons('../data/pseudo/concatenated.fasta')\n",
    "%time i_c = load_condons('../data/pseudo/concatenated_naive_impute.fasta')\n",
    "\n",
    "# 1.5 minutes\n",
    "d = {}\n",
    "for label, content in o_c.iteritems():\n",
    "    d.update(content.value_counts().to_dict())\n",
    "d_sorted = dict(sorted(d.items(), key=lambda x: x[1], reverse=True))\n",
    "mapping = {key: i for i, key in enumerate(d_sorted.keys())}\n",
    "\n",
    "import json\n",
    "with open('../data/pseudo/preprocess/others/condon_mapping.json', 'w') as output:\n",
    "    json.dump(mapping, output, indent='\\t')\n",
    "\n",
    "import json\n",
    "with open('../data/pseudo/preprocess/others/condon_mapping.json', 'r') as input_:\n",
    "    mapping = json.load(input_)\n",
    "\n",
    "# 22 seconds\n",
    "%time o_c_ = o_c.applymap(lambda x: mapping[x])\n",
    "%time i_c_ = i_c.applymap(lambda x: mapping[x])\n",
    "np.save('../data/pseudo/preprocess/o_c_-_-.npy', o_c_)\n",
    "np.save('../data/pseudo/preprocess/i_c_-_-.npy', i_c_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_c_ = np.load('../data/pseudo/preprocess/o_c_-_-.npy')\n",
    "i_c_ = np.load('../data/pseudo/preprocess/i_c_-_-.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove based on SNP counts\n",
    "similar to variance threshold but seems better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 minutes\n",
    "%time variant_counts_o = o_c.apply(pd.Series.value_counts, axis=0)\n",
    "%time variant_counts_i = i_c.apply(pd.Series.value_counts, axis=0)\n",
    "np.save('../data/pseudo/preprocess/others/variant_counts_o.npy', variant_counts_o)\n",
    "np.save('../data/pseudo/preprocess/others/variant_counts_i.npy', variant_counts_i)\n",
    "\n",
    "variant_counts_o = pd.DataFrame(np.load('../data/pseudo/preprocess/others/variant_counts_o.npy'))\n",
    "variant_counts_i = pd.DataFrame(np.load('../data/pseudo/preprocess/others/variant_counts_i.npy'))\n",
    "\n",
    "# True     85753\n",
    "variant_max_counts_o = variant_counts_o.max()\n",
    "(pd.Series(variant_max_counts_o<121)).value_counts()\n",
    "\n",
    "# True      56191\n",
    "variant_max_counts_i = variant_counts_i.max()\n",
    "(variant_max_counts_i<121).value_counts()\n",
    "\n",
    "o_c_v = o_c_[mask][:, variant_max_counts_o<121]\n",
    "i_c_v = i_c_[mask][:, variant_max_counts_i<121]\n",
    "np.save('../data/pseudo/preprocess/o_c_v_-.npy', o_c_v)\n",
    "np.save('../data/pseudo/preprocess/i_c_v_-.npy', i_c_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\chi^2$ on the previous result\n",
    "because some features are all 0's, so gives `divide by 0` warning\n",
    "\n",
    "no warning if we remove those features (on the previous step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "o_c_x = SelectKBest(chi2, k=85753//2).fit_transform(o_c_v, records['toby'][mask].astype('i4'))\n",
    "i_c_x = SelectKBest(chi2, k=56191//2).fit_transform(i_c_v, records['toby'][mask].astype('i4'))\n",
    "\n",
    "np.save('../data/pseudo/preprocess/o_c_x_-.npy', o_c_x)\n",
    "np.save('../data/pseudo/preprocess/i_c_x_-.npy', i_c_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_c_v = np.load('../data/pseudo/preprocess/o_c_v_-.npy')\n",
    "i_c_v = np.load('../data/pseudo/preprocess/i_c_v_-.npy')\n",
    "o_c_x = np.load('../data/pseudo/preprocess/o_c_x_-.npy')\n",
    "i_c_x = np.load('../data/pseudo/preprocess/i_c_x_-.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strkernel.mismatch_kernel import MismatchKernel\n",
    "\n",
    "%time o_c__s = MismatchKernel(l=125, k=2, m=1).get_kernel(o_c_)\n",
    "%time i_c__s = MismatchKernel(l=125, k=2, m=1).get_kernel(i_c_)\n",
    "np.save('../data/pseudo/preprocess/o_c_-_s.npy', o_c__s.kernel)\n",
    "np.save('../data/pseudo/preprocess/i_c_-_s.npy', i_c__s.kernel)\n",
    "\n",
    "%time o_c_v_s = MismatchKernel(l=125, k=2, m=1).get_kernel(o_c_v)\n",
    "%time i_c_v_s = MismatchKernel(l=125, k=2, m=1).get_kernel(i_c_v)\n",
    "np.save('../data/pseudo/preprocess/o_c_v_s.npy', o_c_v_s.kernel)\n",
    "np.save('../data/pseudo/preprocess/i_c_v_s.npy', i_c_v_s.kernel)\n",
    "\n",
    "%time o_c_x_s = MismatchKernel(l=125, k=2, m=1).get_kernel(o_c_x)\n",
    "%time i_c_x_s = MismatchKernel(l=125, k=2, m=1).get_kernel(i_c_x)\n",
    "np.save('../data/pseudo/preprocess/o_c_x_s.npy', o_c_x_s.kernel)\n",
    "np.save('../data/pseudo/preprocess/i_c_x_s.npy', i_c_x_s.kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%time o_c__p = PCA(n_components=119).fit_transform(o_c_)\n",
    "%time i_c__p = PCA(n_components=119).fit_transform(i_c_)\n",
    "np.save('../data/pseudo/preprocess/o_c_-_p.npy', o_c__p)\n",
    "np.save('../data/pseudo/preprocess/i_c_-_p.npy', i_c__p)\n",
    "\n",
    "%time o_c_v_p = PCA(n_components=119).fit_transform(o_c_v)\n",
    "%time i_c_v_p = PCA(n_components=119).fit_transform(i_c_v)\n",
    "np.save('../data/pseudo/preprocess/o_c_v_p.npy', o_c_v_p)\n",
    "np.save('../data/pseudo/preprocess/i_c_v_p.npy', i_c_v_p)\n",
    "\n",
    "%time o_c_x_p = PCA(n_components=119).fit_transform(o_c_x)\n",
    "%time i_c_x_p = PCA(n_components=119).fit_transform(i_c_x)\n",
    "np.save('../data/pseudo/preprocess/o_c_x_p.npy', o_c_x_p)\n",
    "np.save('../data/pseudo/preprocess/i_c_x_p.npy', i_c_x_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "%time o_c__t = TSNE(n_components=3).fit_transform(o_c_)\n",
    "%time i_c__t = TSNE(n_components=3).fit_transform(i_c_)\n",
    "np.save('../data/pseudo/preprocess/o_c_-_t.npy', o_c__t)\n",
    "np.save('../data/pseudo/preprocess/i_c_-_t.npy', i_c__t)\n",
    "\n",
    "%time o_c_v_t = TSNE(n_components=3).fit_transform(o_c_v)\n",
    "%time i_c_v_t = TSNE(n_components=3).fit_transform(i_c_v)\n",
    "np.save('../data/pseudo/preprocess/o_c_v_t.npy', o_c_v_t)\n",
    "np.save('../data/pseudo/preprocess/i_c_v_t.npy', i_c_v_t)\n",
    "\n",
    "%time o_c_x_t = TSNE(n_components=3).fit_transform(o_c_x)\n",
    "%time i_c_x_t = TSNE(n_components=3).fit_transform(i_c_x)\n",
    "np.save('../data/pseudo/preprocess/o_c_x_t.npy', o_c_x_t)\n",
    "np.save('../data/pseudo/preprocess/i_c_x_t.npy', i_c_x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify all possible combinations are created\n",
    "import os\n",
    "d = os.listdir('../data/pseudo/preprocess/')\n",
    "s = {'{}_{}_{}_{}.npy'.format(impute, c_or_n, selection, extraction) for impute in 'io' for c_or_n in 'nc' for selection in '-vx' for extraction in '-pts'}\n",
    "s - set(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "s = {'{}_c_{}_{}.npy'.format(impute, selection, extraction)\n",
    "     for impute in 'io'\n",
    "     for selection in '-vx'\n",
    "     for extraction in '-pts'}\n",
    "\n",
    "data_u = {d: np.load(os.path.join('../data/pseudo/preprocess', d)) for d in s}\n",
    "# mask all data to remove x with NAN labels\n",
    "for k, v in data_u.items():\n",
    "    if v.shape[0] != 119:\n",
    "        data_u[k] = v[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "for file, X in data.items():\n",
    "    encoder = OneHotEncoder(categories='auto', sparse=False, dtype=np.int32)\n",
    "    %time X_encode = encoder.fit_transform(X)\n",
    "    \n",
    "    np.save(os.path.join('../data/pseudo/preprocess/onehot', file), X_encode)\n",
    "    with open(os.path.join('../data/pseudo/preprocess/onehot-encoder', file[:file.index('.')]), 'wb') as output:\n",
    "        pickle.dump(encoder, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_e = {d: np.load(os.path.join('../data/pseudo/preprocess/onehot', d)) for d in s}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y = records['carb'][mask].astype('?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'class_weight': [None, 'balanced', {0:1, 1:4}, {0:1, 1:8}, {0:1, 1:32}, {0:1, 1:64}, {0:1, 1:128}],\n",
    "              'l1_ratio': [0., 0.2, 0.4, 0.6, 0.8, 1.]}\n",
    "clf = GridSearchCV(LogisticRegression(penalty='elasticnet', solver='saga', max_iter=2000, verbose=1, n_jobs=5),\n",
    "                   param_grid=param_grid,\n",
    "                   scoring=['recall', 'balanced_accuracy'],\n",
    "                   refit='balanced_accuracy',\n",
    "                   cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_c_x_t.npy: 0.7222222222222222\n",
      "[[20  8]\n",
      " [ 2  6]]\n",
      "o_c_-_p.npy: 0.6388888888888888\n",
      "[[16 12]\n",
      " [ 1  7]]\n",
      "i_c_x_p.npy: 0.5833333333333334\n",
      "[[15 13]\n",
      " [ 2  6]]\n",
      "i_c_v_t.npy: 0.25\n",
      "[[ 4 24]\n",
      " [ 3  5]]\n",
      "i_c_x_s.npy: 0.6388888888888888\n",
      "[[19  9]\n",
      " [ 4  4]]\n",
      "i_c_x_-.npy: 0.7222222222222222\n",
      "[[24  4]\n",
      " [ 6  2]]\n",
      "o_c_x_s.npy: 0.6666666666666666\n",
      "[[18 10]\n",
      " [ 2  6]]\n",
      "o_c_-_t.npy: 0.4166666666666667\n",
      "[[15 13]\n",
      " [ 8  0]]\n",
      "o_c_v_-.npy: 0.7777777777777778\n",
      "[[22  6]\n",
      " [ 2  6]]\n",
      "o_c_v_p.npy: 0.6388888888888888\n",
      "[[16 12]\n",
      " [ 1  7]]\n",
      "o_c_v_t.npy: 0.7222222222222222\n",
      "[[21  7]\n",
      " [ 3  5]]\n",
      "i_c_v_s.npy: 0.6666666666666666\n",
      "[[19  9]\n",
      " [ 3  5]]\n",
      "i_c_-_p.npy: 0.5555555555555556\n",
      "[[14 14]\n",
      " [ 2  6]]\n",
      "i_c_-_-.npy: 0.7777777777777778\n",
      "[[24  4]\n",
      " [ 4  4]]\n",
      "o_c_x_-.npy: 0.7777777777777778\n",
      "[[22  6]\n",
      " [ 2  6]]\n",
      "o_c_-_s.npy: 0.75\n",
      "[[21  7]\n",
      " [ 2  6]]\n",
      "o_c_x_p.npy: 0.6388888888888888\n",
      "[[16 12]\n",
      " [ 1  7]]\n",
      "i_c_v_-.npy: 0.7222222222222222\n",
      "[[23  5]\n",
      " [ 5  3]]\n",
      "i_c_x_t.npy: 0.5833333333333334\n",
      "[[17 11]\n",
      " [ 4  4]]\n",
      "o_c_-_-.npy: 0.75\n",
      "[[22  6]\n",
      " [ 3  5]]\n",
      "o_c_v_s.npy: 0.6944444444444444\n",
      "[[19  9]\n",
      " [ 2  6]]\n",
      "i_c_v_p.npy: 0.5833333333333334\n",
      "[[15 13]\n",
      " [ 2  6]]\n",
      "i_c_-_t.npy: 0.3888888888888889\n",
      "[[11 17]\n",
      " [ 5  3]]\n",
      "i_c_-_s.npy: 0.5555555555555556\n",
      "[[15 13]\n",
      " [ 3  5]]\n"
     ]
    }
   ],
   "source": [
    "model_u_logistic = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, stratify=y, train_size=0.7)\n",
    "    clf = LogisticRegression(penalty='none', class_weight='balanced',\n",
    "                             solver='lbfgs', max_iter=2000, n_jobs=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "    model_u_logistic[d] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_c_x_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_v_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_x_s.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_x_-.npy: 0.75\n",
      "[[24  4]\n",
      " [ 5  3]]\n",
      "o_c_x_s.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_-_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_v_-.npy: 0.8055555555555556\n",
      "[[23  5]\n",
      " [ 2  6]]\n",
      "o_c_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_v_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_v_s.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_-_-.npy: 0.75\n",
      "[[25  3]\n",
      " [ 6  2]]\n",
      "o_c_x_-.npy: 0.7777777777777778\n",
      "[[22  6]\n",
      " [ 2  6]]\n",
      "o_c_-_s.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_v_-.npy: 0.7777777777777778\n",
      "[[24  4]\n",
      " [ 4  4]]\n",
      "i_c_x_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_-_-.npy: 0.8055555555555556\n",
      "[[24  4]\n",
      " [ 3  5]]\n",
      "o_c_v_s.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_-_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_-_s.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n"
     ]
    }
   ],
   "source": [
    "model_e_logistic = {}\n",
    "for d, X in data_e.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, stratify=y, train_size=0.7)\n",
    "    clf = LogisticRegression(penalty='none', class_weight='balanced',\n",
    "                             solver='lbfgs', max_iter=2000, n_jobs=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "    model_e_logistic[d] = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_c_x_t.npy: 0.7777777777777778\n",
      "[[27  1]\n",
      " [ 7  1]]\n",
      "o_c_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_v_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_x_s.npy: 0.7222222222222222\n",
      "[[26  2]\n",
      " [ 8  0]]\n",
      "i_c_x_-.npy: 0.7777777777777778\n",
      "[[26  2]\n",
      " [ 6  2]]\n",
      "o_c_x_s.npy: 0.8055555555555556\n",
      "[[26  2]\n",
      " [ 5  3]]\n",
      "o_c_-_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_v_-.npy: 0.7777777777777778\n",
      "[[27  1]\n",
      " [ 7  1]]\n",
      "o_c_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_v_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_v_s.npy: 0.8055555555555556\n",
      "[[28  0]\n",
      " [ 7  1]]\n",
      "i_c_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_-_-.npy: 0.7777777777777778\n",
      "[[26  2]\n",
      " [ 6  2]]\n",
      "o_c_x_-.npy: 0.7777777777777778\n",
      "[[27  1]\n",
      " [ 7  1]]\n",
      "o_c_-_s.npy: 0.8055555555555556\n",
      "[[26  2]\n",
      " [ 5  3]]\n",
      "o_c_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_v_-.npy: 0.7777777777777778\n",
      "[[26  2]\n",
      " [ 6  2]]\n",
      "i_c_x_t.npy: 0.75\n",
      "[[26  2]\n",
      " [ 7  1]]\n",
      "o_c_-_-.npy: 0.75\n",
      "[[26  2]\n",
      " [ 7  1]]\n",
      "o_c_v_s.npy: 0.8055555555555556\n",
      "[[26  2]\n",
      " [ 5  3]]\n",
      "i_c_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_-_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_-_s.npy: 0.7777777777777778\n",
      "[[27  1]\n",
      " [ 7  1]]\n"
     ]
    }
   ],
   "source": [
    "model_u_random = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, stratify=y, train_size=0.7)\n",
    "    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, class_weight='balanced')\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "    model_u_random[d] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_c_x_t.npy: 0.7777777777777778\n",
      "[[27  1]\n",
      " [ 7  1]]\n",
      "o_c_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_v_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_x_s.npy: 0.7222222222222222\n",
      "[[26  2]\n",
      " [ 8  0]]\n",
      "i_c_x_-.npy: 0.7777777777777778\n",
      "[[26  2]\n",
      " [ 6  2]]\n",
      "o_c_x_s.npy: 0.8055555555555556\n",
      "[[26  2]\n",
      " [ 5  3]]\n",
      "o_c_-_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_v_-.npy: 0.7777777777777778\n",
      "[[27  1]\n",
      " [ 7  1]]\n",
      "o_c_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_v_t.npy: 0.75\n",
      "[[27  1]\n",
      " [ 8  0]]\n",
      "i_c_v_s.npy: 0.8055555555555556\n",
      "[[28  0]\n",
      " [ 7  1]]\n",
      "i_c_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_-_-.npy: 0.7777777777777778\n",
      "[[26  2]\n",
      " [ 6  2]]\n",
      "o_c_x_-.npy: 0.7777777777777778\n",
      "[[27  1]\n",
      " [ 7  1]]\n",
      "o_c_-_s.npy: 0.8055555555555556\n",
      "[[26  2]\n",
      " [ 5  3]]\n",
      "o_c_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_v_-.npy: 0.7777777777777778\n",
      "[[26  2]\n",
      " [ 6  2]]\n",
      "i_c_x_t.npy: 0.75\n",
      "[[27  1]\n",
      " [ 8  0]]\n",
      "o_c_-_-.npy: 0.75\n",
      "[[26  2]\n",
      " [ 7  1]]\n",
      "o_c_v_s.npy: 0.8055555555555556\n",
      "[[26  2]\n",
      " [ 5  3]]\n",
      "i_c_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_-_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_-_s.npy: 0.7777777777777778\n",
      "[[27  1]\n",
      " [ 7  1]]\n"
     ]
    }
   ],
   "source": [
    "model_e_random = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, stratify=y, train_size=0.7)\n",
    "    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, class_weight='balanced')\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "    model_e_random[d] = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_c_x_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_v_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_x_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "i_c_x_-.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_x_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "o_c_-_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_v_-.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_v_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_v_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "i_c_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_-_-.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_x_-.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_-_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "o_c_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_v_-.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_x_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_-_-.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_v_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "i_c_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_-_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_-_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n"
     ]
    }
   ],
   "source": [
    "model_u_svm = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, stratify=y, train_size=0.7)\n",
    "    clf = SVC(gamma='auto', class_weight='balanced')\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "    model_u_svm[d] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_c_x_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_v_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_x_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "i_c_x_-.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_x_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "o_c_-_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_v_-.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_v_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_v_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "i_c_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_-_-.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_x_-.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_-_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "o_c_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_v_-.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_x_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_-_-.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_c_v_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "i_c_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_-_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_c_-_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n"
     ]
    }
   ],
   "source": [
    "model_e_svm = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, stratify=y, train_size=0.7)\n",
    "    clf = SVC(gamma='auto', class_weight='balanced')\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "    model_e_svm[d] = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = records['carb_num'][mask]\n",
    "X = data_u['i_c_v_-.npy']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, train_size=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_c_x_t.npy: -0.06236691803697636\n",
      "o_c_-_p.npy: -0.1616166231717644\n",
      "i_c_x_p.npy: -0.7516873737948402\n",
      "i_c_v_t.npy: -0.046520717838766545\n",
      "i_c_x_s.npy: -52.804848710674165\n",
      "i_c_x_-.npy: -0.7634902330970657\n",
      "o_c_x_s.npy: -12.043767033444734\n",
      "o_c_-_t.npy: 0.0031379546348111464\n",
      "o_c_v_-.npy: -0.1457025264115981\n",
      "o_c_v_p.npy: -0.1336045007027069\n",
      "o_c_v_t.npy: -0.09286239716640687\n",
      "i_c_v_s.npy: -41.16346551093732\n",
      "i_c_-_p.npy: -0.6959725926467928\n",
      "i_c_-_-.npy: -0.6866939142203072\n",
      "o_c_x_-.npy: -0.23725683493187688\n",
      "o_c_-_s.npy: -10.815286898441316\n",
      "o_c_x_p.npy: -0.23768100366691902\n",
      "i_c_v_-.npy: -0.7061418255484204\n",
      "i_c_x_t.npy: -0.044110352006261344\n",
      "o_c_-_-.npy: -0.15649042254701828\n",
      "o_c_v_s.npy: -4.970325111333776\n",
      "i_c_v_p.npy: -0.6967222031624971\n",
      "i_c_-_t.npy: 0.019653437690379305\n",
      "i_c_-_s.npy: -71.91066692863257\n"
     ]
    }
   ],
   "source": [
    "model_u_linear = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, train_size=0.7)\n",
    "    clf = LinearRegression(n_jobs=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    model_u_linear[d] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_c_x_t.npy: -0.06236691803697636\n",
      "o_c_-_p.npy: -0.1616166231717644\n",
      "i_c_x_p.npy: -0.7516873737948402\n",
      "i_c_v_t.npy: -0.046520717838766545\n",
      "i_c_x_s.npy: -52.804848710674165\n",
      "i_c_x_-.npy: -0.7634902330970657\n",
      "o_c_x_s.npy: -12.043767033444734\n",
      "o_c_-_t.npy: 0.0031379546348111464\n",
      "o_c_v_-.npy: -0.1457025264115981\n",
      "o_c_v_p.npy: -0.1336045007027069\n",
      "o_c_v_t.npy: -0.09286239716640687\n",
      "i_c_v_s.npy: -41.16346551093732\n",
      "i_c_-_p.npy: -0.6959725926467928\n",
      "i_c_-_-.npy: -0.6866939142203072\n",
      "o_c_x_-.npy: -0.23725683493187688\n",
      "o_c_-_s.npy: -10.815286898441316\n",
      "o_c_x_p.npy: -0.23768100366691902\n",
      "i_c_v_-.npy: -0.7061418255484204\n",
      "i_c_x_t.npy: -0.044110352006261344\n",
      "o_c_-_-.npy: -0.15649042254701828\n",
      "o_c_v_s.npy: -4.970325111333776\n",
      "i_c_v_p.npy: -0.6967222031624971\n",
      "i_c_-_t.npy: 0.019653437690379305\n",
      "i_c_-_s.npy: -71.91066692863257\n"
     ]
    }
   ],
   "source": [
    "model_e_linear = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, train_size=0.7)\n",
    "    clf = LinearRegression(n_jobs=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    model_e_linear[d] = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_c_x_t.npy: -0.09737593316606463\n",
      "o_c_-_p.npy: -0.1322842534395765\n",
      "i_c_x_p.npy: -0.03854408183297031\n",
      "i_c_v_t.npy: -0.11639450119937145\n",
      "i_c_x_s.npy: -0.18070000716865664\n",
      "i_c_x_-.npy: -0.2384167250268825\n",
      "o_c_x_s.npy: 0.02466407973751683\n",
      "o_c_-_t.npy: 0.05117599481650981\n",
      "o_c_v_-.npy: -0.1935143872728775\n",
      "o_c_v_p.npy: 0.08448985624086669\n",
      "o_c_v_t.npy: -0.5281207610907386\n",
      "i_c_v_s.npy: -0.04596060293914905\n",
      "i_c_-_p.npy: -0.08601197816316986\n",
      "i_c_-_-.npy: -0.1813005795583007\n",
      "o_c_x_-.npy: -0.2111609532107308\n",
      "o_c_-_s.npy: -0.9910387390884778\n",
      "o_c_x_p.npy: 0.0035553260360088323\n",
      "i_c_v_-.npy: -0.24095694692437042\n",
      "i_c_x_t.npy: -0.09328442581819174\n",
      "o_c_-_-.npy: -0.20460981278778023\n",
      "o_c_v_s.npy: -0.5991909637431416\n",
      "i_c_v_p.npy: -0.005574371832694602\n",
      "i_c_-_t.npy: -0.3728404500813369\n",
      "i_c_-_s.npy: -0.05593235380076633\n"
     ]
    }
   ],
   "source": [
    "model_u_rr = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, train_size=0.7)\n",
    "    clf = RandomForestRegressor(n_estimators=500, n_jobs=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    model_u_rr[d] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_c_x_t.npy: -0.1115954061043869\n",
      "o_c_-_p.npy: -0.13487763015247167\n",
      "i_c_x_p.npy: -0.06578505522622624\n",
      "i_c_v_t.npy: -0.1272774503846259\n",
      "i_c_x_s.npy: -0.14928058010973588\n",
      "i_c_x_-.npy: -0.20304404753370653\n",
      "o_c_x_s.npy: 0.022372111831040264\n",
      "o_c_-_t.npy: 0.05791313937522402\n",
      "o_c_v_-.npy: -0.20945897477184383\n",
      "o_c_v_p.npy: 0.09925719639361441\n",
      "o_c_v_t.npy: -0.5274312145358295\n",
      "i_c_v_s.npy: -0.07454944972290378\n",
      "i_c_-_p.npy: -0.09297292144806857\n",
      "i_c_-_-.npy: -0.20443909244809633\n",
      "o_c_x_-.npy: -0.17117404725798901\n",
      "o_c_-_s.npy: -0.9797854263420553\n",
      "o_c_x_p.npy: 0.015096239763985775\n",
      "i_c_v_-.npy: -0.23545545827014802\n",
      "i_c_x_t.npy: -0.06769170332791097\n",
      "o_c_-_-.npy: -0.2101866583583778\n",
      "o_c_v_s.npy: -0.6444252342220629\n",
      "i_c_v_p.npy: -0.0006828394496678492\n",
      "i_c_-_t.npy: -0.3355599125423918\n",
      "i_c_-_s.npy: -0.04616066205299263\n"
     ]
    }
   ],
   "source": [
    "model_e_rr = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, train_size=0.7)\n",
    "    clf = RandomForestRegressor(n_estimators=500, n_jobs=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    model_e_rr[d] = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_c_x_t.npy: -0.07849453836880826\n",
      "o_c_-_p.npy: -0.07849453836880826\n",
      "i_c_x_p.npy: -0.07849453836880826\n",
      "i_c_v_t.npy: -0.07849453836880826\n",
      "i_c_x_s.npy: -0.07807466984972877\n",
      "i_c_x_-.npy: -0.07849243855588028\n",
      "o_c_x_s.npy: -0.07809771255883113\n",
      "o_c_-_t.npy: -0.07849453836880826\n",
      "o_c_v_-.npy: -0.07849455641444947\n",
      "o_c_v_p.npy: -0.07849453836880826\n",
      "o_c_v_t.npy: -0.07849453836880826\n",
      "i_c_v_s.npy: -0.07806637720859144\n",
      "i_c_-_p.npy: -0.07849453836880826\n",
      "i_c_-_-.npy: -0.07897358015607425\n",
      "o_c_x_-.npy: -0.0784945383756761\n",
      "o_c_-_s.npy: -0.07801058474447276\n",
      "o_c_x_p.npy: -0.07849453836880826\n",
      "i_c_v_-.npy: -0.07851038991206472\n",
      "i_c_x_t.npy: -0.07849453836880826\n",
      "o_c_-_-.npy: -0.07850324051490443\n",
      "o_c_v_s.npy: -0.07803351988881957\n",
      "i_c_v_p.npy: -0.07849453836880826\n",
      "i_c_-_t.npy: -0.07849453836880826\n",
      "i_c_-_s.npy: -0.07803630371112602\n"
     ]
    }
   ],
   "source": [
    "model_u_svr = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, train_size=0.7)\n",
    "    clf = SVR(gamma='auto')\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    model_u_svr[d] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_c_x_t.npy: -0.07849453836880826\n",
      "o_c_-_p.npy: -0.07849453836880826\n",
      "i_c_x_p.npy: -0.07849453836880826\n",
      "i_c_v_t.npy: -0.07849453836880826\n",
      "i_c_x_s.npy: -0.07807466984972877\n",
      "i_c_x_-.npy: -0.07849243855588028\n",
      "o_c_x_s.npy: -0.07809771255883113\n",
      "o_c_-_t.npy: -0.07849453836880826\n",
      "o_c_v_-.npy: -0.07849455641444947\n",
      "o_c_v_p.npy: -0.07849453836880826\n",
      "o_c_v_t.npy: -0.07849453836880826\n",
      "i_c_v_s.npy: -0.07806637720859144\n",
      "i_c_-_p.npy: -0.07849453836880826\n",
      "i_c_-_-.npy: -0.07897358015607425\n",
      "o_c_x_-.npy: -0.0784945383756761\n",
      "o_c_-_s.npy: -0.07801058474447276\n",
      "o_c_x_p.npy: -0.07849453836880826\n",
      "i_c_v_-.npy: -0.07851038991206472\n",
      "i_c_x_t.npy: -0.07849453836880826\n",
      "o_c_-_-.npy: -0.07850324051490443\n",
      "o_c_v_s.npy: -0.07803351988881957\n",
      "i_c_v_p.npy: -0.07849453836880826\n",
      "i_c_-_t.npy: -0.07849453836880826\n",
      "i_c_-_s.npy: -0.07803630371112602\n"
     ]
    }
   ],
   "source": [
    "model_e_svr = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, train_size=0.7)\n",
    "    clf = SVR(gamma='auto')\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    model_e_svr[d] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
