{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from load import load_pseudo, load_nucleotides\n",
    "\n",
    "pd.options.display.precision = 3\n",
    "pd.options.display.max_colwidth = 10\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.33 s, sys: 127 ms, total: 5.46 s\n",
      "Wall time: 5.49 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>missing</th>\n",
       "      <th>missing_%</th>\n",
       "      <th>sequence_i</th>\n",
       "      <th>missing_i</th>\n",
       "      <th>missing_%_i</th>\n",
       "      <th>carb</th>\n",
       "      <th>toby</th>\n",
       "      <th>carb_num</th>\n",
       "      <th>toby_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>TA151</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>31842</td>\n",
       "      <td>6.588</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>28410</td>\n",
       "      <td>5.878</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>IC1</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>46071</td>\n",
       "      <td>9.532</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>34714</td>\n",
       "      <td>7.182</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A237</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>44514</td>\n",
       "      <td>9.210</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>35933</td>\n",
       "      <td>7.434</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5920</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>49497</td>\n",
       "      <td>10.241</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>36873</td>\n",
       "      <td>7.629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>LiA96</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>44067</td>\n",
       "      <td>9.117</td>\n",
       "      <td>ATGAGT...</td>\n",
       "      <td>34454</td>\n",
       "      <td>7.128</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   sequence  missing  missing_% sequence_i  missing_i  missing_%_i  \\\n",
       "0  TA151  ATGAGT...    31842      6.588  ATGAGT...      28410      5.878     \n",
       "1    IC1  ATGAGT...    46071      9.532  ATGAGT...      34714      7.182     \n",
       "2   A237  ATGAGT...    44514      9.210  ATGAGT...      35933      7.434     \n",
       "3   5920  ATGAGT...    49497     10.241  ATGAGT...      36873      7.629     \n",
       "4  LiA96  ATGAGT...    44067      9.117  ATGAGT...      34454      7.128     \n",
       "\n",
       "    carb   toby  carb_num  toby_num  \n",
       "0   True  False      -2.0      16.0  \n",
       "1  False  False       2.0      14.0  \n",
       "2   True  False      -1.0       4.0  \n",
       "3    NaN    NaN       NaN       NaN  \n",
       "4  False  False       0.0      18.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time records = load_pseudo()\n",
    "numerical_response = pd.read_csv('../data/pseudo/Perron_phenotype-GSU-training.csv')\n",
    "records = records.merge(numerical_response[['strain', 'carb.lag.delta', 'toby.lag.delta']],\n",
    "                        left_on='lab-id', right_on='strain', how='left')\n",
    "records.rename(columns={'carb.lag.delta': 'carb_num', 'toby.lag.delta': 'toby_num'}, inplace=True)\n",
    "records.drop(columns=['strain', 'lab-id'], inplace=True)\n",
    "records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (records['toby'].notna() & records['carb'].notna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_n = np.load('../data/pseudo/preprocess/o_n_-_-.npy')\n",
    "i_n = np.load('../data/pseudo/preprocess/i_n_-_-.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40 seconds\n",
    "%time o_n = load_nucleotides('../data/pseudo/concatenated.fasta')\n",
    "%time i_n = load_nucleotides('../data/pseudo/concatenated_naive_impute.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 45 seconds\n",
    "forward = str.maketrans('-ACTG', '01234')\n",
    "def transformation(str):\n",
    "    return [int(i) for i in str.translate(forward)]\n",
    "%time o_n = pd.DataFrame(records['sequence'].apply(transformation).to_list())\n",
    "%time i_n = pd.DataFrame(records['sequence_i'].apply(transformation).to_list())\n",
    "np.save('../data/pseudo/preprocess/o_n_-_-.npy', o_n)\n",
    "np.save('../data/pseudo/preprocess/i_n_-_-.npy', i_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold(0.01)\n",
    "\n",
    "# justification(not rigorous) for why < 0.016 is the threshold to drop a column\n",
    "a, b = 4, 3\n",
    "arr = np.ones((122, 1))*a\n",
    "arr[:2] = b\n",
    "np.var(arr)\n",
    "\n",
    "o_n_v = selector.fit_transform(o_n)\n",
    "o_n_v_selected = pd.Series(selector.get_support())\n",
    "o_n_v_selected.value_counts()\n",
    "\n",
    "i_n_v = selector.fit_transform(i_n)\n",
    "i_n_v_selected = pd.Series(selector.get_support())\n",
    "i_n_v_selected.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove based on SNP counts\n",
    "similar to variance threshold but seems better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# less than 6 min\n",
    "%time snp_counts_o = o_n.apply(pd.Series.value_counts, axis=0)\n",
    "%time snp_counts_i = i_n.apply(pd.Series.value_counts, axis=0)\n",
    "np.save('../data/pseudo/preprocess/others/snp_counts_o.npy', snp_counts_o.to_numpy())\n",
    "np.save('../data/pseudo/preprocess/others/snp_counts_i.npy', snp_counts_i.to_numpy())\n",
    "\n",
    "snp_counts_o = pd.DataFrame(np.load('../data/pseudo/preprocess/others/snp_counts_o.npy'))\n",
    "snp_counts_i = pd.DataFrame(np.load('../data/pseudo/preprocess/others/snp_counts_i.npy'))\n",
    "\n",
    "# True     204101\n",
    "snp_max_counts_o = snp_counts_o.max()\n",
    "(snp_max_counts_o<121).value_counts()\n",
    "\n",
    "# True     ???\n",
    "snp_max_counts_i = snp_counts_i.max()\n",
    "(snp_max_counts_i<121).value_counts()\n",
    "\n",
    "o_n_v = o_n[mask].loc[:, (snp_max_counts_o<121)]\n",
    "i_n_v = i_n[mask].loc[:, (snp_max_counts_i<121)]\n",
    "np.save('../data/pseudo/preprocess/o_n_v_-.npy', o_n_v)\n",
    "np.save('../data/pseudo/preprocess/i_n_v_-.npy', i_n_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\chi^2$ on the previous result\n",
    "because some features are all 0's, so gives `divide by 0` warning\n",
    "\n",
    "no warning if we remove those features (on the previous step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "o_n_x = SelectKBest(chi2, k=250032//2).fit_transform(o_n_v, records['toby'][mask].astype('i4'))\n",
    "i_n_x = SelectKBest(chi2, k=105124//2).fit_transform(i_n_v, records['toby'][mask].astype('i4'))\n",
    "np.save('../data/pseudo/preprocess/o_n_x_-.npy', o_n_x)\n",
    "np.save('../data/pseudo/preprocess/i_n_x_-.npy', i_n_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_n_v = np.load('../data/pseudo/preprocess/o_n_v_-.npy')\n",
    "i_n_v = np.load('../data/pseudo/preprocess/i_n_v_-.npy')\n",
    "o_n_x = np.load('../data/pseudo/preprocess/o_n_x_-.npy')\n",
    "i_n_x = np.load('../data/pseudo/preprocess/i_n_x_-.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strkernel.mismatch_kernel import MismatchKernel\n",
    "\n",
    "# 9 minutes\n",
    "%time o_n__s = MismatchKernel(l=5, k=4, m=1).get_kernel(o_n)\n",
    "%time i_n__s = MismatchKernel(l=5, k=4, m=1).get_kernel(i_n)\n",
    "np.save('../data/pseudo/preprocess/o_n_-_s.npy', o_n__s.kernel)\n",
    "np.save('../data/pseudo/preprocess/i_n_-_s.npy', i_n__s.kernel)\n",
    "\n",
    "# 3 and 1 minutes\n",
    "%time o_n_v_s = MismatchKernel(l=5, k=4, m=1).get_kernel(o_n_v)\n",
    "%time i_n_v_s = MismatchKernel(l=5, k=4, m=1).get_kernel(i_n_v)\n",
    "np.save('../data/pseudo/preprocess/o_n_v_s.npy', o_n_v_s.kernel)\n",
    "np.save('../data/pseudo/preprocess/i_n_v_s.npy', i_n_v_s.kernel)\n",
    "\n",
    "# 2 minutes\n",
    "%time o_n_x_s = MismatchKernel(l=5, k=4, m=1).get_kernel(o_n_x)\n",
    "%time i_n_x_s = MismatchKernel(l=5, k=4, m=1).get_kernel(i_n_x)\n",
    "np.save('../data/pseudo/preprocess/o_n_x_s.npy', o_n_x_s.kernel)\n",
    "np.save('../data/pseudo/preprocess/i_n_x_s.npy', i_n_x_s.kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%time o_n__p = PCA(n_components=119).fit_transform(o_n)\n",
    "%time i_n__p = PCA(n_components=119).fit_transform(i_n)\n",
    "np.save('../data/pseudo/preprocess/o_n_-_p.npy', o_n__p)\n",
    "np.save('../data/pseudo/preprocess/i_n_-_p.npy', i_n__p)\n",
    "\n",
    "%time o_n_v_p = PCA(n_components=119).fit_transform(o_n_v)\n",
    "%time i_n_v_p = PCA(n_components=119).fit_transform(i_n_v)\n",
    "np.save('../data/pseudo/preprocess/o_n_v_p.npy', o_n_v_p)\n",
    "np.save('../data/pseudo/preprocess/i_n_v_p.npy', i_n_v_p)\n",
    "\n",
    "%time o_n_x_p = PCA(n_components=119).fit_transform(o_n_x)\n",
    "%time i_n_x_p = PCA(n_components=119).fit_transform(i_n_x)\n",
    "np.save('../data/pseudo/preprocess/o_n_x_p.npy', o_n_x_p)\n",
    "np.save('../data/pseudo/preprocess/i_n_x_p.npy', i_n_x_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "%time o_n__t = TSNE(n_components=3).fit_transform(o_n)\n",
    "%time i_n__t = TSNE(n_components=3).fit_transform(i_n)\n",
    "np.save('../data/pseudo/preprocess/o_n_-_t.npy', o_n__t)\n",
    "np.save('../data/pseudo/preprocess/i_n_-_t.npy', i_n__t)\n",
    "\n",
    "%time o_n_v_t = TSNE(n_components=3).fit_transform(o_n_v)\n",
    "%time i_n_v_t = TSNE(n_components=3).fit_transform(i_n_v)\n",
    "np.save('../data/pseudo/preprocess/o_n_v_t.npy', o_n_v_t)\n",
    "np.save('../data/pseudo/preprocess/i_n_v_t.npy', i_n_v_t)\n",
    "\n",
    "%time o_n_x_t = TSNE(n_components=3).fit_transform(o_n_x)\n",
    "%time i_n_x_t = TSNE(n_components=3).fit_transform(i_n_x)\n",
    "np.save('../data/pseudo/preprocess/o_n_x_t.npy', o_n_x_t)\n",
    "np.save('../data/pseudo/preprocess/i_n_x_t.npy', i_n_x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "s = {'{}_n_{}_{}.npy'.format(impute, selection, extraction)\n",
    "     for impute in 'io'\n",
    "     for selection in '-vx'\n",
    "     for extraction in '-pts'}\n",
    "\n",
    "data_u = {d: np.load(os.path.join('../data/pseudo/preprocess', d)) for d in s}\n",
    "# mask all data to remove x with NAN labels\n",
    "for k, v in data_u.items():\n",
    "    if v.shape[0] != 119:\n",
    "        data_u[k] = v[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "for file, X in data.items():\n",
    "    encoder = OneHotEncoder(categories='auto', sparse=False, dtype=np.int32)\n",
    "    %time X_encode = encoder.fit_transform(X)\n",
    "    \n",
    "    np.save(os.path.join('../data/pseudo/preprocess/onehot', file), X_encode)\n",
    "    with open(os.path.join('../data/pseudo/preprocess/onehot-encoder', file[:file.index('.')]), 'wb') as output:\n",
    "        pickle.dump(encoder, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_e = {d: np.load(os.path.join('../data/pseudo/preprocess/onehot', d)) for d in s}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = records['carb'][mask].astype('?')\n",
    "X = data_u['i_n_v_-.npy']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, stratify=y, train_size=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'class_weight': [None, 'balanced', {0:1, 1:4}, {0:1, 1:8}, {0:1, 1:32}, {0:1, 1:64}, {0:1, 1:128}],\n",
    "              'l1_ratio': [0., 0.2, 0.4, 0.6, 0.8, 1.]}\n",
    "clf = GridSearchCV(LogisticRegression(penalty='elasticnet', solver='saga', max_iter=2000, verbose=1, n_jobs=5),\n",
    "                   param_grid=param_grid,\n",
    "                   scoring=['recall', 'balanced_accuracy'],\n",
    "                   refit='balanced_accuracy',\n",
    "                   cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28  0]\n",
      " [ 8  0]]\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty='none', class_weight='balanced',\n",
    "                         solver='lbfgs', max_iter=2000, n_jobs=5)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_n_x_-.npy: 0.7777777777777778\n",
      "[[25  3]\n",
      " [ 5  3]]\n",
      "o_n_-_t.npy: 0.6944444444444444\n",
      "[[22  6]\n",
      " [ 5  3]]\n",
      "i_n_-_-.npy: 0.6944444444444444\n",
      "[[24  4]\n",
      " [ 7  1]]\n",
      "o_n_v_-.npy: 0.7777777777777778\n",
      "[[22  6]\n",
      " [ 2  6]]\n",
      "o_n_-_p.npy: 0.6666666666666666\n",
      "[[16 12]\n",
      " [ 0  8]]\n",
      "i_n_v_t.npy: 0.6666666666666666\n",
      "[[18 10]\n",
      " [ 2  6]]\n",
      "o_n_v_s.npy: 0.6944444444444444\n",
      "[[19  9]\n",
      " [ 2  6]]\n",
      "o_n_x_-.npy: 0.8333333333333334\n",
      "[[24  4]\n",
      " [ 2  6]]\n",
      "i_n_x_p.npy: 0.6111111111111112\n",
      "[[17 11]\n",
      " [ 3  5]]\n",
      "i_n_v_s.npy: 0.6388888888888888\n",
      "[[17 11]\n",
      " [ 2  6]]\n",
      "i_n_x_s.npy: 0.6388888888888888\n",
      "[[17 11]\n",
      " [ 2  6]]\n",
      "i_n_-_p.npy: 0.5833333333333334\n",
      "[[16 12]\n",
      " [ 3  5]]\n",
      "o_n_x_p.npy: 0.6111111111111112\n",
      "[[15 13]\n",
      " [ 1  7]]\n",
      "o_n_x_s.npy: 0.6388888888888888\n",
      "[[17 11]\n",
      " [ 2  6]]\n",
      "o_n_x_t.npy: 0.4166666666666667\n",
      "[[13 15]\n",
      " [ 6  2]]\n",
      "i_n_v_p.npy: 0.6111111111111112\n",
      "[[17 11]\n",
      " [ 3  5]]\n",
      "i_n_v_-.npy: 0.75\n",
      "[[25  3]\n",
      " [ 6  2]]\n",
      "o_n_-_s.npy: 0.75\n",
      "[[21  7]\n",
      " [ 2  6]]\n",
      "o_n_-_-.npy: 0.8055555555555556\n",
      "[[25  3]\n",
      " [ 4  4]]\n",
      "i_n_x_t.npy: 0.4722222222222222\n",
      "[[15 13]\n",
      " [ 6  2]]\n",
      "i_n_-_t.npy: 0.5555555555555556\n",
      "[[17 11]\n",
      " [ 5  3]]\n",
      "o_n_v_t.npy: 0.5833333333333334\n",
      "[[15 13]\n",
      " [ 2  6]]\n",
      "o_n_v_p.npy: 0.6111111111111112\n",
      "[[15 13]\n",
      " [ 1  7]]\n",
      "i_n_-_s.npy: 0.5555555555555556\n",
      "[[15 13]\n",
      " [ 3  5]]\n"
     ]
    }
   ],
   "source": [
    "model_u_logistic = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, stratify=y, train_size=0.7)\n",
    "    clf = LogisticRegression(penalty='none', class_weight='balanced',\n",
    "                             solver='lbfgs', max_iter=2000, n_jobs=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "    model_u_logistic[d] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_n_x_-.npy: 0.7222222222222222\n",
      "[[23  5]\n",
      " [ 5  3]]\n",
      "o_n_-_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_-_-.npy: 0.7777777777777778\n",
      "[[25  3]\n",
      " [ 5  3]]\n",
      "o_n_v_-.npy: 0.75\n",
      "[[22  6]\n",
      " [ 3  5]]\n",
      "o_n_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_v_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_v_s.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_x_-.npy: 0.8333333333333334\n",
      "[[25  3]\n",
      " [ 3  5]]\n",
      "i_n_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_v_s.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_x_s.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_x_s.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_x_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_v_-.npy: 0.7222222222222222\n",
      "[[23  5]\n",
      " [ 5  3]]\n",
      "o_n_-_s.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_-_-.npy: 0.7777777777777778\n",
      "[[22  6]\n",
      " [ 2  6]]\n",
      "i_n_x_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_-_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_v_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_-_s.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n"
     ]
    }
   ],
   "source": [
    "model_e_logistic = {}\n",
    "for d, X in data_e.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, stratify=y, train_size=0.7)\n",
    "    clf = LogisticRegression(penalty='none', class_weight='balanced',\n",
    "                             solver='lbfgs', max_iter=2000, n_jobs=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "    model_e_logistic[d] = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[28,  0],\n",
       "       [ 8,  0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500, n_jobs=5, class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "confusion_matrix(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_n_x_-.npy: 0.7777777777777778\n",
      "[[26  2]\n",
      " [ 6  2]]\n",
      "o_n_-_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_-_-.npy: 0.7777777777777778\n",
      "[[26  2]\n",
      " [ 6  2]]\n",
      "o_n_v_-.npy: 0.8333333333333334\n",
      "[[27  1]\n",
      " [ 5  3]]\n",
      "o_n_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_v_t.npy: 0.7777777777777778\n",
      "[[27  1]\n",
      " [ 7  1]]\n",
      "o_n_v_s.npy: 0.75\n",
      "[[25  3]\n",
      " [ 6  2]]\n",
      "o_n_x_-.npy: 0.8055555555555556\n",
      "[[27  1]\n",
      " [ 6  2]]\n",
      "i_n_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_v_s.npy: 0.75\n",
      "[[27  1]\n",
      " [ 8  0]]\n",
      "i_n_x_s.npy: 0.7222222222222222\n",
      "[[26  2]\n",
      " [ 8  0]]\n",
      "i_n_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_x_s.npy: 0.7222222222222222\n",
      "[[25  3]\n",
      " [ 7  1]]\n",
      "o_n_x_t.npy: 0.75\n",
      "[[27  1]\n",
      " [ 8  0]]\n",
      "i_n_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_v_-.npy: 0.8055555555555556\n",
      "[[27  1]\n",
      " [ 6  2]]\n",
      "o_n_-_s.npy: 0.8055555555555556\n",
      "[[26  2]\n",
      " [ 5  3]]\n",
      "o_n_-_-.npy: 0.75\n",
      "[[26  2]\n",
      " [ 7  1]]\n",
      "i_n_x_t.npy: 0.75\n",
      "[[27  1]\n",
      " [ 8  0]]\n",
      "i_n_-_t.npy: 0.7222222222222222\n",
      "[[26  2]\n",
      " [ 8  0]]\n",
      "o_n_v_t.npy: 0.7222222222222222\n",
      "[[26  2]\n",
      " [ 8  0]]\n",
      "o_n_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_-_s.npy: 0.7222222222222222\n",
      "[[26  2]\n",
      " [ 8  0]]\n"
     ]
    }
   ],
   "source": [
    "model_u_random = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, stratify=y, train_size=0.7)\n",
    "    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, class_weight='balanced')\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "    model_u_random[d] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_n_x_-.npy: 0.7777777777777778\n",
      "[[26  2]\n",
      " [ 6  2]]\n",
      "o_n_-_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_-_-.npy: 0.7777777777777778\n",
      "[[26  2]\n",
      " [ 6  2]]\n",
      "o_n_v_-.npy: 0.7777777777777778\n",
      "[[27  1]\n",
      " [ 7  1]]\n",
      "o_n_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_v_t.npy: 0.7777777777777778\n",
      "[[27  1]\n",
      " [ 7  1]]\n",
      "o_n_v_s.npy: 0.75\n",
      "[[25  3]\n",
      " [ 6  2]]\n",
      "o_n_x_-.npy: 0.8333333333333334\n",
      "[[27  1]\n",
      " [ 5  3]]\n",
      "i_n_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_v_s.npy: 0.75\n",
      "[[27  1]\n",
      " [ 8  0]]\n",
      "i_n_x_s.npy: 0.7222222222222222\n",
      "[[26  2]\n",
      " [ 8  0]]\n",
      "i_n_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_x_s.npy: 0.7222222222222222\n",
      "[[25  3]\n",
      " [ 7  1]]\n",
      "o_n_x_t.npy: 0.75\n",
      "[[27  1]\n",
      " [ 8  0]]\n",
      "i_n_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_v_-.npy: 0.7777777777777778\n",
      "[[26  2]\n",
      " [ 6  2]]\n",
      "o_n_-_s.npy: 0.8055555555555556\n",
      "[[26  2]\n",
      " [ 5  3]]\n",
      "o_n_-_-.npy: 0.7777777777777778\n",
      "[[27  1]\n",
      " [ 7  1]]\n",
      "i_n_x_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_-_t.npy: 0.75\n",
      "[[27  1]\n",
      " [ 8  0]]\n",
      "o_n_v_t.npy: 0.7222222222222222\n",
      "[[26  2]\n",
      " [ 8  0]]\n",
      "o_n_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_-_s.npy: 0.75\n",
      "[[27  1]\n",
      " [ 8  0]]\n"
     ]
    }
   ],
   "source": [
    "model_e_random = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, stratify=y, train_size=0.7)\n",
    "    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, class_weight='balanced')\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "    model_e_random[d] = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_n_x_-.npy: 0.7777777777777778\n",
      "[[23  5]\n",
      " [ 3  5]]\n",
      "o_n_-_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_-_-.npy: 0.8055555555555556\n",
      "[[25  3]\n",
      " [ 4  4]]\n",
      "o_n_v_-.npy: 0.8888888888888888\n",
      "[[25  3]\n",
      " [ 1  7]]\n",
      "o_n_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_v_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_v_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "o_n_x_-.npy: 0.8888888888888888\n",
      "[[26  2]\n",
      " [ 2  6]]\n",
      "i_n_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_v_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "i_n_x_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "i_n_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_x_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "o_n_x_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_v_-.npy: 0.8055555555555556\n",
      "[[23  5]\n",
      " [ 2  6]]\n",
      "o_n_-_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "o_n_-_-.npy: 0.8611111111111112\n",
      "[[25  3]\n",
      " [ 2  6]]\n",
      "i_n_x_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_-_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_v_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_-_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n"
     ]
    }
   ],
   "source": [
    "model_u_svm = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, stratify=y, train_size=0.7)\n",
    "    clf = SVC(gamma='auto', class_weight='balanced')\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "    model_u_svm[d] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_n_x_-.npy: 0.7777777777777778\n",
      "[[23  5]\n",
      " [ 3  5]]\n",
      "o_n_-_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_-_-.npy: 0.8055555555555556\n",
      "[[25  3]\n",
      " [ 4  4]]\n",
      "o_n_v_-.npy: 0.8888888888888888\n",
      "[[25  3]\n",
      " [ 1  7]]\n",
      "o_n_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_v_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_v_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "o_n_x_-.npy: 0.8888888888888888\n",
      "[[26  2]\n",
      " [ 2  6]]\n",
      "i_n_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_v_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "i_n_x_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "i_n_-_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_x_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_x_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "o_n_x_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_v_-.npy: 0.8055555555555556\n",
      "[[23  5]\n",
      " [ 2  6]]\n",
      "o_n_-_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n",
      "o_n_-_-.npy: 0.8611111111111112\n",
      "[[25  3]\n",
      " [ 2  6]]\n",
      "i_n_x_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_-_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_v_t.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "o_n_v_p.npy: 0.7777777777777778\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "i_n_-_s.npy: 0.2222222222222222\n",
      "[[ 0 28]\n",
      " [ 0  8]]\n"
     ]
    }
   ],
   "source": [
    "model_e_svm = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, stratify=y, train_size=0.7)\n",
    "    clf = SVC(gamma='auto', class_weight='balanced')\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "    model_e_svm[d] = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = records['carb_num'][mask]\n",
    "X = data_u['i_n_v_-.npy']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, train_size=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5576861056531992"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression(n_jobs=5)\n",
    "reg.fit(X_train, y_train)\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_n_x_-.npy: -1.0429679767140527\n",
      "o_n_-_t.npy: -0.25335024886382285\n",
      "i_n_-_-.npy: -0.9181924822179699\n",
      "o_n_v_-.npy: -0.13730328170158757\n",
      "o_n_-_p.npy: -0.16533320987865197\n",
      "i_n_v_t.npy: -0.04703137946065428\n",
      "o_n_v_s.npy: -4.322237131402667\n",
      "o_n_x_-.npy: -0.2556004451938423\n",
      "i_n_x_p.npy: -1.039553922398774\n",
      "i_n_v_s.npy: -20.476127914659585\n",
      "i_n_x_s.npy: -11.23674680450402\n",
      "i_n_-_p.npy: -0.9199558897108064\n",
      "o_n_x_p.npy: -0.2632762012651939\n",
      "o_n_x_s.npy: -4.4642080691905885\n",
      "o_n_x_t.npy: -0.014700652051022667\n",
      "i_n_v_p.npy: -0.9499575747410811\n",
      "i_n_v_-.npy: -0.950169715214543\n",
      "o_n_-_s.npy: -7.7157630320248565\n",
      "o_n_-_-.npy: -0.1603923407681085\n",
      "i_n_x_t.npy: -0.13770547164200098\n",
      "i_n_-_t.npy: 0.006902683747397997\n",
      "o_n_v_t.npy: -0.042306774904844024\n",
      "o_n_v_p.npy: -0.13803847963604432\n",
      "i_n_-_s.npy: -13.098631740901306\n"
     ]
    }
   ],
   "source": [
    "model_u_linear = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, train_size=0.7)\n",
    "    clf = LinearRegression(n_jobs=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    model_u_linear[d] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_n_x_-.npy: -1.0429679767140527\n",
      "o_n_-_t.npy: -0.25335024886382285\n",
      "i_n_-_-.npy: -0.9181924822179699\n",
      "o_n_v_-.npy: -0.13730328170158757\n",
      "o_n_-_p.npy: -0.16533320987865197\n",
      "i_n_v_t.npy: -0.04703137946065428\n",
      "o_n_v_s.npy: -4.322237131402667\n",
      "o_n_x_-.npy: -0.2556004451938423\n",
      "i_n_x_p.npy: -1.039553922398774\n",
      "i_n_v_s.npy: -20.476127914659585\n",
      "i_n_x_s.npy: -11.23674680450402\n",
      "i_n_-_p.npy: -0.9199558897108064\n",
      "o_n_x_p.npy: -0.2632762012651939\n",
      "o_n_x_s.npy: -4.4642080691905885\n",
      "o_n_x_t.npy: -0.014700652051022667\n",
      "i_n_v_p.npy: -0.9499575747410811\n",
      "i_n_v_-.npy: -0.950169715214543\n",
      "o_n_-_s.npy: -7.7157630320248565\n",
      "o_n_-_-.npy: -0.1603923407681085\n",
      "i_n_x_t.npy: -0.13770547164200098\n",
      "i_n_-_t.npy: 0.006902683747397997\n",
      "o_n_v_t.npy: -0.042306774904844024\n",
      "o_n_v_p.npy: -0.13803847963604432\n",
      "i_n_-_s.npy: -13.098631740901306\n"
     ]
    }
   ],
   "source": [
    "model_e_linear = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, train_size=0.7)\n",
    "    clf = LinearRegression(n_jobs=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    model_e_linear[d] = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_n_x_-.npy: -0.20303416537538976\n",
      "o_n_-_t.npy: -0.20613788469491845\n",
      "i_n_-_-.npy: -0.22399553833852637\n",
      "o_n_v_-.npy: -0.23145226005679786\n",
      "o_n_-_p.npy: -0.059301730072513825\n",
      "i_n_v_t.npy: -0.11012481568281474\n",
      "o_n_v_s.npy: -0.570538177727536\n",
      "o_n_x_-.npy: -0.26012765413989913\n",
      "i_n_x_p.npy: 0.002157851277950873\n",
      "i_n_v_s.npy: -0.16117861336127248\n",
      "i_n_x_s.npy: -0.06020384427472503\n",
      "i_n_-_p.npy: -0.1262951731781965\n",
      "o_n_x_p.npy: -0.13501035639251158\n",
      "o_n_x_s.npy: -0.052760277261573174\n",
      "o_n_x_t.npy: -0.17121698640712513\n",
      "i_n_v_p.npy: -0.005869143235269769\n",
      "i_n_v_-.npy: -0.20461427742700367\n",
      "o_n_-_s.npy: -1.158306895254901\n",
      "o_n_-_-.npy: -0.22106844324354125\n",
      "i_n_x_t.npy: -0.19325903333425254\n",
      "i_n_-_t.npy: 0.011403695276958281\n",
      "o_n_v_t.npy: -0.16882134020789086\n",
      "o_n_v_p.npy: -0.0648949353993773\n",
      "i_n_-_s.npy: -0.06631627979817489\n"
     ]
    }
   ],
   "source": [
    "model_u_rr = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, train_size=0.7)\n",
    "    clf = RandomForestRegressor(n_estimators=500, n_jobs=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    model_u_rr[d] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_n_x_-.npy: -0.1925639154098544\n",
      "o_n_-_t.npy: -0.22575066960765433\n",
      "i_n_-_-.npy: -0.20201676759767295\n",
      "o_n_v_-.npy: -0.2612391656786788\n",
      "o_n_-_p.npy: -0.08617507248614564\n",
      "i_n_v_t.npy: -0.14167842212357673\n",
      "o_n_v_s.npy: -0.5747468773884035\n",
      "o_n_x_-.npy: -0.24548701723234734\n",
      "i_n_x_p.npy: -0.02062880377181653\n",
      "i_n_v_s.npy: -0.1756157687281148\n",
      "i_n_x_s.npy: -0.039723899969671184\n",
      "i_n_-_p.npy: -0.08192058198461516\n",
      "o_n_x_p.npy: -0.10542525484573617\n",
      "o_n_x_s.npy: -0.0729427364415891\n",
      "o_n_x_t.npy: -0.16866643965921324\n",
      "i_n_v_p.npy: -0.011340009705257836\n",
      "i_n_v_-.npy: -0.20520039085720554\n",
      "o_n_-_s.npy: -1.2100129233229477\n",
      "o_n_-_-.npy: -0.2662107389781905\n",
      "i_n_x_t.npy: -0.21137328958614843\n",
      "i_n_-_t.npy: 0.009126489067799004\n",
      "o_n_v_t.npy: -0.1299208254983597\n",
      "o_n_v_p.npy: -0.048218848824064686\n",
      "i_n_-_s.npy: -0.0681710614574429\n"
     ]
    }
   ],
   "source": [
    "model_e_rr = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, train_size=0.7)\n",
    "    clf = RandomForestRegressor(n_estimators=500, n_jobs=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    model_e_rr[d] = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_n_x_-.npy: -0.08651327032195755\n",
      "o_n_-_t.npy: -0.07849453836880826\n",
      "i_n_-_-.npy: -0.08941467144290449\n",
      "o_n_v_-.npy: -0.08118517992701979\n",
      "o_n_-_p.npy: -0.07849453836879694\n",
      "i_n_v_t.npy: -0.07849453836880826\n",
      "o_n_v_s.npy: -0.07800730304654402\n",
      "o_n_x_-.npy: -0.08080126595184267\n",
      "i_n_x_p.npy: -0.07849470358760224\n",
      "i_n_v_s.npy: -0.07808437765706344\n",
      "i_n_x_s.npy: -0.07805524445569256\n",
      "i_n_-_p.npy: -0.07863266017010906\n",
      "o_n_x_p.npy: -0.07849453836880826\n",
      "o_n_x_s.npy: -0.07815699098170126\n",
      "o_n_x_t.npy: -0.07849453836880826\n",
      "i_n_v_p.npy: -0.07849290209618665\n",
      "i_n_v_-.npy: -0.091714455611138\n",
      "o_n_-_s.npy: -0.07801840342626942\n",
      "o_n_-_-.npy: -0.08426548690742375\n",
      "i_n_x_t.npy: -0.07849453836880826\n",
      "i_n_-_t.npy: -0.07849453836880826\n",
      "o_n_v_t.npy: -0.07849453836880826\n",
      "o_n_v_p.npy: -0.07849453836880826\n",
      "i_n_-_s.npy: -0.07803530310792328\n"
     ]
    }
   ],
   "source": [
    "model_u_svr = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, train_size=0.7)\n",
    "    clf = SVR(gamma='auto')\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    model_u_svr[d] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_n_x_-.npy: -0.08651327032195755\n",
      "o_n_-_t.npy: -0.07849453836880826\n",
      "i_n_-_-.npy: -0.08941467144290449\n",
      "o_n_v_-.npy: -0.08118517992701979\n",
      "o_n_-_p.npy: -0.07849453836879694\n",
      "i_n_v_t.npy: -0.07849453836880826\n",
      "o_n_v_s.npy: -0.07800730304654402\n",
      "o_n_x_-.npy: -0.08080126595184267\n",
      "i_n_x_p.npy: -0.07849470358760224\n",
      "i_n_v_s.npy: -0.07808437765706344\n",
      "i_n_x_s.npy: -0.07805524445569256\n",
      "i_n_-_p.npy: -0.07863266017010906\n",
      "o_n_x_p.npy: -0.07849453836880826\n",
      "o_n_x_s.npy: -0.07815699098170126\n",
      "o_n_x_t.npy: -0.07849453836880826\n",
      "i_n_v_p.npy: -0.07849290209618665\n",
      "i_n_v_-.npy: -0.091714455611138\n",
      "o_n_-_s.npy: -0.07801840342626942\n",
      "o_n_-_-.npy: -0.08426548690742375\n",
      "i_n_x_t.npy: -0.07849453836880826\n",
      "i_n_-_t.npy: -0.07849453836880826\n",
      "o_n_v_t.npy: -0.07849453836880826\n",
      "o_n_v_p.npy: -0.07849453836880826\n",
      "i_n_-_s.npy: -0.07803530310792328\n"
     ]
    }
   ],
   "source": [
    "model_e_svr = {}\n",
    "for d, X in data_u.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, train_size=0.7)\n",
    "    clf = SVR(gamma='auto')\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('{}: {}'.format(d, clf.score(X_test, y_test)))\n",
    "    model_e_svr[d] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
